{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion Week 3: PySpark Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Comprehensions, Lambdas, Generators, and Yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# List comprehensions:\n",
    "x = range(100)\n",
    "\n",
    "y = [n**2 for n in x if n < 5]\n",
    "\n",
    "print y\n",
    "\n",
    "y2 = [n**2 if n % 2 else 0 for n in y]\n",
    "print y2\n",
    "\n",
    "print [a * b for a in y for b in y2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lambda Expressions\n",
    "\n",
    "def convert_me(n):\n",
    "    return 1./ n ** 2\n",
    "\n",
    "convert_you = lambda x: 1./x ** 2\n",
    "\n",
    "convert_me(10) == convert_you(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen1 = lambda n: (i for i in range(n))\n",
    "\n",
    "def gen2(n):\n",
    "    for i in range(n):\n",
    "        yield i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g1 = gen1(5)\n",
    "g2 = gen2(5)\n",
    "for i in range(5):\n",
    "    print next(g1) == next(g2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDDs\n",
    "\n",
    "Spark revolves around the concept of a resilient distributed dataset (RDD), which is a fault-tolerant collection of elements that can be operated on in parallel. There are two ways to create RDDs: parallelizing an existing collection in your driver program, or referencing a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "n = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load new RDD using a collection from 0 to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd1 = n.parallelize(range(10), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`rdd.collect()` return a list that contains all of the elements in this RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of making our own collection, let's load in a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd2 = n.textFile('link_text.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a peek at the first few entries in this document - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Here is the Raw document\"\n",
    "\n",
    "!head -n 5 link_text.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rdd2.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do something interesting with this data - get the domains of all of the websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_site(iterator):\n",
    "    for link in iterator:\n",
    "        index = link.find(\"www.\")\n",
    "        end = link.find(\".com\")\n",
    "        if index > 0 and end > 0:\n",
    "            yield link[index + 4: end]\n",
    "\n",
    "site_rdd = rdd2.mapPartitionsWithIndex(lambda index, iterator: get_site(iterator))\n",
    "# notice how i toss out the index\n",
    "# also notice how nothing happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "site_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print rdd2.getNumPartitions(), rdd2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice how the object itself is not very eventful..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the raw implementation of `rdd.distinct` from PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distinct(self, numPartitions=None):\n",
    "    \"\"\"\n",
    "    Return a new RDD containing the distinct elements in this RDD.\n",
    "\n",
    "    >>> sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())\n",
    "    [1, 2, 3]\n",
    "    \"\"\"\n",
    "    return self.map(lambda x: (x, None)) \\\n",
    "               .reduceByKey(lambda x, _: x, numPartitions) \\\n",
    "               .map(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us get the distinct URLs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "site_rdd.distinct().take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rest is left as an optional exercise - can you guess what's going on below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "redrdd = n.parallelize(range(24), 4)\n",
    "testrdd = redrdd.map(lambda x: ((x ** 2 * 7) % 13 , x ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def checkit(rdd):\n",
    "    def p(n, itr):\n",
    "        for i in itr:\n",
    "            yield (n, i)\n",
    "    return rdd.mapPartitionsWithIndex(p).collect()\n",
    "\n",
    "testrdd = testrdd.reduceByKey(lambda x, y: x + y, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "checkit(testrdd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
